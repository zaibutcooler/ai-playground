{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01e8ca-883e-465a-b73b-d2c4913299a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c350420d-cd33-463e-aefe-a21eb172a56a",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "Need to stick with utf-8 (most efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7743ed80-1965-4d96-b4da-e4be3bfd55d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello\"\n",
    "list(text.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57bd3033-c27b-4e17-8a27-e8e5966c7072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "japanese = \"こんにちは\"\n",
    "list(japanese.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5b0ca-f9e1-4616-954d-2906a35d4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e13219-977c-412e-ae11-0a69ec043333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7684022-5eb0-4330-9c45-5e1c970071b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "508686a6-2cb9-404f-af4c-6aa3419a147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "    \n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        print(\"tb->\",text_bytes)\n",
    "        ids = list(text_bytes)\n",
    "\n",
    "        \n",
    "    def encode(self,text):\n",
    "        pass\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        pass\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a1b547-82ae-4e7d-ab1d-ccde5211149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb-> b'hello world bro'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.train(\"hello world bro\",512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec8bc0-8118-43d6-8a77-9e4dfc010585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d0ba4-706e-4f70-bfa4-bd01a0519638",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def save(self, file_prefix):\n",
    "    #     \"\"\"\n",
    "    #     Saves two files: file_prefix.vocab and file_prefix.model\n",
    "    #     This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "    #     - model file is the critical one, intended for load()\n",
    "    #     - vocab file is just a pretty printed version for human inspection only\n",
    "    #     \"\"\"\n",
    "    #     # write the model: to be used in load() later\n",
    "    #     model_file = file_prefix + \".model\"\n",
    "    #     with open(model_file, 'w') as f:\n",
    "    #         # write the version, pattern and merges, that's all that's needed\n",
    "    #         f.write(\"minbpe v1\\n\")\n",
    "    #         f.write(f\"{self.pattern}\\n\")\n",
    "    #         # write the special tokens, first the number of them, then each one\n",
    "    #         f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "    #         for special, idx in self.special_tokens.items():\n",
    "    #             f.write(f\"{special} {idx}\\n\")\n",
    "    #         # the merges dict\n",
    "    #         for idx1, idx2 in self.merges:\n",
    "    #             f.write(f\"{idx1} {idx2}\\n\")\n",
    "    #     # write the vocab: for the human to look at\n",
    "    #     vocab_file = file_prefix + \".vocab\"\n",
    "    #     inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "    #     with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    #         for idx, token in self.vocab.items():\n",
    "    #             # note: many tokens may be partial utf-8 sequences\n",
    "    #             # and cannot be decoded into valid strings. Here we're using\n",
    "    #             # errors='replace' to replace them with the replacement char �.\n",
    "    #             # this also means that we couldn't possibly use .vocab in load()\n",
    "    #             # because decoding in this way is a lossy operation!\n",
    "    #             s = render_token(token)\n",
    "    #             # find the children of this token, if any\n",
    "    #             if idx in inverted_merges:\n",
    "    #                 # if this token has children, render it nicely as a merge\n",
    "    #                 idx0, idx1 = inverted_merges[idx]\n",
    "    #                 s0 = render_token(self.vocab[idx0])\n",
    "    #                 s1 = render_token(self.vocab[idx1])\n",
    "    #                 f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "    #             else:\n",
    "    #                 # otherwise this is leaf token, just print it\n",
    "    #                 # (this should just be the first 256 tokens, the bytes)\n",
    "    #                 f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    # def load(self, model_file):\n",
    "    #     \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "    #     assert model_file.endswith(\".model\")\n",
    "    #     # read the model file\n",
    "    #     merges = {}\n",
    "    #     special_tokens = {}\n",
    "    #     idx = 256\n",
    "    #     with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "    #         # read the version\n",
    "    #         version = f.readline().strip()\n",
    "    #         assert version == \"minbpe v1\"\n",
    "    #         # read the pattern\n",
    "    #         self.pattern = f.readline().strip()\n",
    "    #         # read the special tokens\n",
    "    #         num_special = int(f.readline().strip())\n",
    "    #         for _ in range(num_special):\n",
    "    #             special, special_idx = f.readline().strip().split()\n",
    "    #             special_tokens[special] = int(special_idx)\n",
    "    #         # read the merges\n",
    "    #         for line in f:\n",
    "    #             idx1, idx2 = map(int, line.split())\n",
    "    #             merges[(idx1, idx2)] = idx\n",
    "    #             idx += 1\n",
    "    #     self.merges = merges\n",
    "    #     self.special_tokens = special_tokens\n",
    "    #     self.vocab = self._build_vocab()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
