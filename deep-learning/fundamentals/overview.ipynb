{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5731efe-7e45-41c7-b8e9-b744eff48c5f",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Neural network is just a function (A giant mathematical expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf09c9e-4923-4b94-adb5-ca25cc258978",
   "metadata": {},
   "source": [
    "Training the network means improving(reducing) the loss function using gradient descent.\n",
    "\n",
    "The initial networks sometimes starts with random numbers (weights and bias)\n",
    "\n",
    "Gradient is a slope aka predictor fo the the function at certain point\n",
    "\n",
    "Back propagation is the process of finding the gradient of the children nodes. Why? For the loss function to improve slightly amount each time. \n",
    "\n",
    "Learning rate is the rate the function will improve in each iterations.\n",
    "\n",
    "Optimizers are algorithms that adjust the learning rate and update the parameters of the neural network based on the computed gradients. They help optimize the training process by efficiently adjusting the parameters towards minimizing the loss. (Just to get the best result and to avoid overshoot and undershoot)\n",
    "\n",
    "Activation functions are functions that takes the input and makes it smoother. (producing small and accurate numbers)\n",
    "\n",
    "We need zero grad so that the gradient will reset to zero. Unless it will use the previous gradient everytime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ee154-c190-44b4-9ebc-d412bd8a7330",
   "metadata": {},
   "source": [
    "#### The Perceptron (single neuron)\n",
    "\n",
    "It consists of x (inputs), w(weights), and b(bias) and y_hat(output)\n",
    "\n",
    "y_hat = activation( sum_of(x_m * w_m) + b â€‹\n",
    "sum) +bias)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de1002d-c63f-46f4-baa1-a671ea41c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = 0\n",
    "\n",
    "x = [0.5, 0.3, 0.2] # Input\n",
    "w = [0.3, 0.7, 0.8] # Weight\n",
    "b = 4 # Bias\n",
    "\n",
    "# simple activation function\n",
    "def activation_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "# summations\n",
    "for m in range(len(x)):\n",
    "    weighted_sum += x[m] * w[m]\n",
    "weighted_sum += b\n",
    "\n",
    "# Output\n",
    "y_hat = activation_function(weighted_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6e874-5bda-4020-852f-e67f78a512a8",
   "metadata": {},
   "source": [
    "#### MultiLayer Perceptron\n",
    "\n",
    "A layer after combining all the neurons\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b43e9-02df-495d-aa45-da7e03494d42",
   "metadata": {},
   "source": [
    "#### Forward Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74a6c6-9738-4345-996c-9eb4bb4535cf",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Gradient descent is a process of improving the loss functions. \n",
    "\n",
    "Eg: Let's say the loss function is a mountain. what gradient descent doing is it is going to the lowest point of loss function. \n",
    "\n",
    "We improve the loss function by increasing a small value (known as learning rate) to the parameters(the weights).\n",
    "\n",
    "Funfact -> Stochastic gradient descent and batch gradient descent exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09f6a2-3fd6-471f-9daa-b50416e063dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1024b592-7a82-4fcf-92ca-52f322468d41",
   "metadata": {},
   "source": [
    "#### Backward Process\n",
    "\n",
    "It is the process of finding the gradient of the loss function and improve the function by little step(learning rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806666fa-8bd8-45f5-b9f9-da817d034ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softmax(x):\n",
    "        pass\n",
    "        \n",
    "    def sigmoid(x):\n",
    "        pass\n",
    "        \n",
    "    def forward(x):\n",
    "        pass\n",
    "\n",
    "    def backward(x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7366a54-e3c8-424f-a48b-51b90324aae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
